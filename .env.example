LLM_MODE=auto

# Local LLM (Ollama/Llama)
LOCAL_MODEL=llama3.1:8b
LOCAL_HOST=http://localhost:11434

# Cloud LLM (Gemini - fallback)
GEMINI_API_KEY=
GEMINI_MODEL=

# Batch Processing - OPTIMIZED for local LLM
# Smaller batches = more reliable, faster per-batch processing
CERTS_PER_API_CALL=5  # Reduced from 20!
MAX_CONCURRENT_REQUESTS=2  # Reduced from 4

# Rate Limiting
REQUEST_DELAY_SECONDS=0.1  # Minimal for local