LLM_MODE=auto

# Local LLM (Ollama/Llama)
LOCAL_MODEL=llama3.1:8b
LOCAL_HOST=http://localhost:11434

# Cloud LLM (Gemini - fallback)
GEMINI_API_KEY=
GEMINI_MODEL=

# Batch Processing - OPTIMIZED for local LLM
# Smaller batches = more reliable, faster per-batch processing
CERTS_PER_API_CALL=5  # Reduced from 20!
MAX_CONCURRENT_REQUESTS=2  # Reduced from 4

# Rate Limiting
REQUEST_DELAY_SECONDS=0.1  # Minimal for local

# Dataset Subset Sampling
ENABLE_SAMPLING=true
SAMPLE_SIZE_PER_CLASS=3000  # Per class (phishing/benign)
SAMPLING_METHOD=random  # 'random', 'stratified', or 'first'
RANDOM_SEED=42  # For reproducibility
